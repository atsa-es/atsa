---
title: "Stationarity & introductory functions"
subtitle: "FISH 507 â€“ Applied Time Series Analysis"
author: "Mark Scheuerell"
date: "10 Jan 2019"
output:
  ioslides_presentation:
    css: lecture_slides.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
set.seed(123)
```

## Topics for today

### Characteristics of time series (ts)
* Expectation, mean & variance
* Covariance & correlation
* Stationarity
* Autocovariance & autocorrelation
* Correlograms
* White noise
* Random walks
* Backshift & difference operators


## Expectation & the mean

The expectation ($E$) of a variable is its mean value in the population

$\text{E}(x) \equiv$ mean of $x = \mu$

We can estimate $\mu$ from a sample as

$$
m = \frac{\sum_{i=1}^N{x_i}}{N}
$$


## Variance

$\text{E}([x - \mu]^2) \equiv$ expected deviations of $x$ about $\mu$

$\text{E}([x - \mu]^2) \equiv$ variance of $x = \sigma^2$

We can estimate $\sigma^2$ from a sample as

$$
s^2 = \frac{1}{N-1}\sum_{i=1}^N{(x_i - m)^2}
$$


## Covariance

If we have two variables, $x$ and $y$, we can generalize variance

$$
\sigma^2 = \text{E}([x_i - \mu][x_i - \mu])
$$

into _covariance_
 
$$
\gamma_{x,y} = \text{E}([x_i - \mu_x][y_i - \mu_y])
$$


## Covariance

If we have two variables, $x$ and $y$, we can generalize variance

$$
\sigma^2 = \text{E}([x_i - \mu][x_i - \mu])
$$

into _covariance_
 
$$
\gamma_{x,y} = \text{E}([x_i - \mu_x][y_i - \mu_y])
$$

We can estimate $\gamma_{x,y}$ from a sample as

$$
\text{Cov}(x,y) = \frac{1}{N-1}\sum_{i=1}^N{(x_i - m_x)(y_i - m_y)}
$$


## Graphical example of covariance

```{r, fig.align="center"}
# create dummy x set
xx <- runif(25, 0, 10)
yy <- 1 + 0.3*xx + rnorm(25,0,1.5)

par(mai=c(1,1,0,0), omi=c(0,0,0.5,1))
plot(xx, yy, pch=16, asp=1, las = 1,
     cex=1.2, cex.lab=1.2, col="black",
     xlab=expression(italic(x)),
     ylab=expression(italic(y)),
     main="")

```


## Graphical example of covariance

```{r, fig.align="center"}
par(mai=c(1,1,0,0), omi=c(0,0,0.5,1))
plot(xx, yy, pch=16, asp=1, las = 1,
     cex=1.2, cex.lab=1.2, col="black",
     xlab=expression(italic(x)),
     ylab=expression(italic(y)),
     main="")

# add mean lines	
abline(h=mean(yy), lty="dashed")
abline(v=mean(xx), lty="dashed")
# add labels for means
mtext(side=3, line=0.4, at=mean(xx), expression(italic(m[x])))
mtext(side=4, line=0.5, at=mean(yy), expression(italic(m[y])), las=1)

# add quadrant labels
mtext(side=3, line=0.4, at=(mean(xx)+par()$usr[1])/2, expression((italic(x[i])-italic(m[x])) < 0))
mtext(side=3, line=0.4, at=(mean(xx)+par()$usr[2])/2, expression((italic(x[i])-italic(m[x])) > 0))
mtext(side=4, line=0.5, at=(mean(yy)+par()$usr[3])/2, expression((italic(y[i])-italic(m[y])) < 0), las=1)
mtext(side=4, line=0.5, at=(mean(yy)+par()$usr[4])/2, expression((italic(y[i])-italic(m[y])) > 0), las=1)
```


## Graphical example of covariance

```{r, fig.align="center"}
par(mai=c(1,1,0,0), omi=c(0,0,0.5,1))
plot(xx, yy, pch=16, asp=1, las = 1,
     cex=1.2, cex.lab=1.2, col="gray",
     xlab=expression(italic(x)),
     ylab=expression(italic(y)),
     main="")

# add mean lines	
abline(h=mean(yy), lty="dashed")
abline(v=mean(xx), lty="dashed")
# add labels for means
mtext(side=3, line=0.4, at=mean(xx), expression(italic(m[x])))
mtext(side=4, line=0.5, at=mean(yy), expression(italic(m[y])), las=1)

# add quadrant labels
mtext(side=3, line=0.4, at=(mean(xx)+par()$usr[1])/2, expression((italic(x[i])-italic(m[x])) < 0))
mtext(side=3, line=0.4, at=(mean(xx)+par()$usr[2])/2, expression((italic(x[i])-italic(m[x])) > 0))
mtext(side=4, line=0.5, at=(mean(yy)+par()$usr[3])/2, expression((italic(y[i])-italic(m[y])) < 0), las=1)
mtext(side=4, line=0.5, at=(mean(yy)+par()$usr[4])/2, expression((italic(y[i])-italic(m[y])) > 0), las=1)

# get indices for data pairs with neg cov
negC <- (xx<mean(xx) & yy>mean(yy)) | (xx>mean(xx) & yy<mean(yy))

# overlay pos & neg cov values
points(xx[negC], yy[negC], pch="-", cex=2, col="darkred")
points(xx[!negC], yy[!negC], pch="+", cex=1.5, col="blue")
```


## Correlation

_Correlation_ is a dimensionless measure of the linear association between 2 variables, $x$ & $y$

It is simply the covariance standardized by the standard deviations

$$
\rho_{x,y} = \frac{\gamma_{x,y}}{\sigma_x \sigma_y}
$$

$$
-1 < \rho_{x,y} < 1
$$


## Correlation

_Correlation_ is a dimensionless measure of the linear association between 2 variables $x$ & $y$

It is simply the covariance standardized by the standard deviations

$$
\rho_{x,y} = \frac{\gamma_{x,y}}{\sigma_x \sigma_y}
$$

We can estimate $\rho_{x,y}$ from a sample as

$$
\text{Cor}(x,y) = \frac{\text{Cov}(x,y)}{s_x s_y}
$$


## Stationarity & the mean

Consider a single value, $x_t$


## Stationarity & the mean

Consider a single value, $x_t$

$\text{E}(x_t)$ is taken across an ensemble of _all_ possible time series


## Stationarity & the mean

```{r station_in_mean}
nn <- 200
tt <- 40
ww <- matrix(rnorm(nn*tt), tt, nn)

mm <- apply(ww, 1, mean)

par(mai = c(0.9,0.9,0.1,0.1), omi = c(0,0,0,0))

matplot(ww, type="l", lty="solid",  las = 1,
        ylab = expression(italic(x[t])), xlab = "Time",
        col = gray(0.5, 0.2))
points(rep(0,tt), pch = "-", col = "blue", cex=1.5)
```


## Stationarity & the mean

```{r ex_ts_plot_joint_dist, fig.cap="Our single realization is our estimate!"}

par(mai = c(0.9,0.9,0.1,0.1), omi = c(0,0,0,0))

matplot(ww, type="l", lty="solid",  las = 1,
        ylab = expression(italic(x[t])), xlab = "Time",
        col = gray(0.5, 0.2))
lines(ww[,1], col = "blue", lwd = 2)
```


## Stationarity & the mean

If $\text{E}(x_t)$ is constant across time, we say the time series is _stationary_ in the mean


## Stationarity of time series

_Stationarity_ is a convenient assumption that allows us to describe the statistical properties of a time series.

In general, a time series is said to be stationary if there is

1. no systematic change in the mean or variance  
2. no systematic trend  
3. no periodic variations or seasonality


## Identifying stationarity

```{r, fig.align="center"}
par(mfrow = c(2,2), mai = c(0.7,0.4,0.1,0.1))

plot.ts(arima.sim(model = list(ar = 0.3, sd = 0.1), n = 100),
        las = 1, ylab = "")
plot.ts(arima.sim(model = list(ar = 0.9, sd = 0.1), n = 100),
        las = 1, ylab = "")
plot.ts(arima.sim(model = list(ar = -0.9, sd = 0.1), n = 100),
        las = 1, ylab = "")
plot.ts(arima.sim(model = list(ma = 0.1, sd = 0.1), n = 100),
        las = 1, ylab = "")
```


## Identifying stationarity

Our eyes are really bad at identifying stationarity, so we will learn some tools to help us


## Autocovariance function (ACVF)

For stationary ts, we define the _autocovariance function_ ($\gamma_k$) as

$$
\gamma_k = \text{E}([x_t - \mu][x_{t+k} - \mu])
$$

which means that

$$
\gamma_0 = \text{E}([x_t - \mu][x_{t} - \mu]) = \sigma^2
$$


## Autocovariance function (ACVF)

For stationary ts, we define the _autocovariance function_ ($\gamma_k$) as

$$
\gamma_k = \text{E}([x_t - \mu][x_{t+k} - \mu])
$$

"Smooth" series have large ACVF for large $k$

"Choppy" series have ACVF near 0 for small $k$


## Autocovariance function (ACVF)

For stationary ts, we define the _autocovariance function_ ($\gamma_k$) as

$$
\gamma_k = \text{E}([x_t - \mu][x_{t+k} - \mu])
$$

We can estimate $\gamma_k$ from a sample as

$$
c_k = \frac{1}{N}\sum_{t=1}^{N-k}{(x_t - m)(x_{t+k} - m)}
$$


## Autocorrelation function (ACF)

The _autocorrelation function_ (ACF) is simply the ACVF normalized by the variance

$$
\rho_k = \frac{\gamma_k}{\sigma^2} = \frac{\gamma_k}{\gamma_0}
$$

The ACF measures the correlation of a time series against a time-shifted version of itself


## Autocorrelation function (ACF)

The _autocorrelation function_ (ACF) is simply the ACVF normalized by the variance

$$
\rho_k = \frac{\gamma_k}{\sigma^2} = \frac{\gamma_k}{\gamma_0}
$$

The ACF measures the correlation of a time series against a time-shifted version of itself

We can estimate ACF from a sample as

$$
r_k = \frac{c_k}{c_0}
$$


## Properties of the ACF

The ACF has several important properties:

- $-1 \leq r_k \leq 1$  

> - $r_k = r_{-k}$  

> - $r_k$ of periodic function is itself periodic  

> - $r_k$ for the sum of 2 independent variables is the sum of $r_k$ for each of them 


## The correlogram 

```{r, fig.cap="Graphical output for the ACF"}
par(mai=c(1,1,0,0), omi=c(0.1,0.1,0.1,0.1))
plot(NA, NA, type="n", xlim=c(0,15), ylim=c(-1,1),
     xlab="", xaxt="n", ylab="", las = 1)
abline(h=0)
axis(side=1, at=seq(15), labels=FALSE)
axis(side=1, at=seq(0,15,5))
mtext(expression(paste("Lag ", (italic(k)))), side=1, line=3, cex=1.2)
mtext(expression(paste("ACF ", (italic(r[k])))), side=2, line=3, cex=1.2)
```


## The correlogram 

```{r, fig.cap="The ACF at lag = 0 is always 1"}
par(mai=c(1,1,0,0), omi=c(0.1,0.1,0.1,0.1))
plot(NA, NA, type="n", xlim=c(0,15), ylim=c(-1,1),
     xlab="", xaxt="n", ylab="", las = 1)
abline(h=0)
axis(side=1, at=seq(15), labels=FALSE)
axis(side=1, at=seq(0,15,5))
mtext(expression(paste("Lag ", (italic(k)))), side=1, line=3, cex=1.2)
mtext(expression(paste("ACF ", (italic(r[k])))), side=2, line=3, cex=1.2)

lines(c(0,0), c(0,1), lwd=2, col="darkred")
text(x=1, y =1, expression(italic(r)[0] == 1), col="darkred")
```


## The correlogram 

```{r, fig.cap="Approximate confidence intervals"}
par(mai=c(1,1,0,0), omi=c(0.1,0.1,0.1,0.1))
plot(NA, NA, type="n", xlim=c(0,15), ylim=c(-1,1),
     xlab="", xaxt="n", ylab="", las = 1)
abline(h=0)
axis(side=1, at=seq(15), labels=FALSE)
axis(side=1, at=seq(0,15,5))
mtext(expression(paste("Lag ", (italic(k)))), side=1, line=3, cex=1.2)
mtext(expression(paste("ACF ", (italic(r[k])))), side=2, line=3, cex=1.2)

lines(c(0,0), c(0,1), lwd=2, col="darkred")
text(x=1, y =1, expression(italic(r)[0] == 1), col="darkred")

# add 95% CI's
nn <- 30
alpha <- 0.05
ts.SD <- qnorm(1-alpha/2, 0, 1)/sqrt(nn)
abline(h=-ts.SD, lty="dashed", col="blue")
text(x=14, y=-0.55, expression(-frac(italic(z)[1-frac(alpha,2)], sqrt(italic(n)))), col="blue")
abline(h=ts.SD, lty="dashed", col="blue")
text(x=14, y=0.55, expression(+frac(italic(z)[1-frac(alpha,2)], sqrt(italic(n)))), col="blue")
```


## ACF for deterministic forms

```{r}
## length of ts
nn <- 100

## trend only
par(mfrow=c(1,2), mai=c(1,1,0,0), omi=c(0.1,0.1,0.6,0.1))
tt <- seq(nn)
plot.ts(tt, ylab=expression(italic(x[t])))
acf(tt)
mtext("Linear trend {1,2,3,...,100}", outer=TRUE, line=1, cex=1.5)
```


## ACF for deterministic forms

```{r}
par(mfrow=c(1,2), mai=c(1,1,0,0), omi=c(0.1,0.1,0.6,0.1))
## compute the 2 predictor variables
tt <- sin(2*pi*seq(nn)/12)
plot.ts(tt, ylab=expression(italic(x[t])))
acf(tt)
mtext("Discrete (monthly) sine wave", outer=TRUE, line=1, cex=1.5)
```


## ACF for deterministic forms

```{r}
par(mfrow=c(1,2), mai=c(1,1,0,0), omi=c(0.1,0.1,0.6,0.1))
## compute the 2 predictor variables
tt <- sin(2*pi*seq(nn)/12) - seq(nn)/50
plot.ts(tt, ylab=expression(italic(x[t])))
acf(tt, lag.max=30)
mtext("Linear trend + seasonal effect", outer=TRUE, line=1, cex=1.5)
```


## ACF for deterministic forms

```{r}
par(mfrow=c(1,2), mai=c(1,1,0,0), omi=c(0.1,0.1,0.6,0.1))
# compute the 2 predictor variables
tt <- rep(floor(runif(nn/10,1,101)), times=10)
plot.ts(tt, ylab=expression(italic(x[t])))
acf(tt)
mtext("Sequence of 10 random numbers repeated 10 times", outer=TRUE, line=1, cex=1.5)
```


## Induced autocorrelation

Recall the transitive property, whereby

If $A = B$ and $B = C$, then $A = C$


## Induced autocorrelation

Recall the transitive property, whereby

If $A = B$ and $B = C$, then $A = C$

which suggests that

If $x \propto y$ and $y \propto z$, then $x \propto z$


## Induced autocorrelation

Recall the transitive property, whereby

If $A = B$ and $B = C$, then $A = C$

which suggests that

If $x \propto y$ and $y \propto z$, then $x \propto z$

and thus

If $x_t \propto x_{t+1}$ and $x_{t+1} \propto x_{t+2}$, then $x_t \propto x_{t+2}$


## Partial autocorrelation funcion (PACF)

The _partial autocorrelation function_ ($\phi_k$) measures the correlation between a series $x_t$ and $x_{t+k}$ with the linear dependence of $\{x_{t-1},x_{t-2},\dots,x_{t-k-1}\}$ removed


## Partial autocorrelation funcion (PACF)

The _partial autocorrelation function_ ($\phi_k$) measures the correlation between a series $x_t$ and $x_{t+k}$ with the linear dependence of $\{x_{t-1},x_{t-2},\dots,x_{t-k-1}\}$ removed


We can estimate $\phi_k$ from a sample as

$$
\phi_k =
    \begin{cases}
      \text{Cor}(x_1,x_0) = \rho_1 & \text{if } k = 1 \\
      \text{Cor}(x_k-x_k^{k-1}, x_0-x_0^{k-1}) & \text{if } k \geq 2
    \end{cases}
$$


$$
x_k^{k-1} = \beta_1 x_{k-1} + \beta_2 x_{k-2} + \dots + \beta_{k-1} x_1
$$

$$
x_0^{k-1} = \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_{k-1} x_{k-1}
$$


## Lake Washington phytoplankton

```{r lwa_phyto_ts}
library(MARSS)
data(lakeWAplankton)
lwa <- lakeWAplanktonTrans
lwa <- lwa[lwa[,"Year"] >= 1975,]
lwa <- ts(lwa, start = c(1975,1), freq = 12)
par(mai=c(1,1,0,0), omi=c(0.1,0.1,0.1,0.1))
plot.ts(lwa[,"Cryptomonas"], ylab=expression(log(italic(Cryptomonus))))
```


## Lake Washington phytoplankton

```{r lwa_phyto_acf, fig.cap = "Autocorrelation"}
par(mai=c(1,1,0,0), omi=c(0.1,0.1,0.1,0.1))
acf(lwa[,"Cryptomonas"], na.action = na.pass)
```


## Lake Washington phytoplankton

```{r lwa_phyto_pacf, fig.cap = "Partial autocorrelation"}
par(mai=c(1,1,0,0), omi=c(0.1,0.1,0.1,0.1))
pacf(lwa[,"Cryptomonas"], na.action = na.pass)
```


## Cross-covariance function

Often we want to look for relationships between 2 different time series

We can extend the notion of covariance to _cross-covariance_


## Cross-covariance function (CCVF)

Often we want to look for relationships between 2 different time series

We can extend the notion of covariance to _cross-covariance_

We can estimate $g^{x,y}_k$ from a sample as

$$
g^{x,y}_k = \frac{1}{N}\sum_{t=1}^{N-k}{(x_t - m_x)(y_{t+k} - m_y)}
$$




