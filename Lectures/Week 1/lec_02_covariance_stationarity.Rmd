---
title: "Stationarity & introductory functions"
subtitle: "FISH 507 â€“ Applied Time Series Analysis"
author: "Mark Scheuerell"
date: "10 Jan 2019"
output:
  ioslides_presentation:
    css: lecture_slides.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
set.seed(123)
```

## Topics for today

Characteristics of time series

* Expectation, mean & variance
* Covariance & correlation
* Stationarity
* Autocovariance & autocorrelation
* Correlograms

White noise

Random walks

Backshift & difference operators


## Expectation & the mean

The expectation ($E$) of a variable is its mean value in the population

$\text{E}(x) \equiv$ mean of $x = \mu$

We can estimate $\mu$ from a sample as

$$
m = \frac{\sum_{i=1}^N{x_i}}{N}
$$


## Variance

$\text{E}([x - \mu]^2) \equiv$ expected deviations of $x$ about $\mu$

$\text{E}([x - \mu]^2) \equiv$ variance of $x = \sigma^2$

We can estimate $\sigma^2$ from a sample as

$$
s^2 = \frac{1}{N-1}\sum_{i=1}^N{(x_i - m)^2}
$$


## Covariance

If we have two variables, $x$ and $y$, we can generalize variance

$$
\sigma^2 = \text{E}([x_i - \mu][x_i - \mu])
$$

into _covariance_
 
$$
\gamma_{x,y} = \text{E}([x_i - \mu_x][y_i - \mu_y])
$$


## Covariance

If we have two variables, $x$ and $y$, we can generalize variance

$$
\sigma^2 = \text{E}([x_i - \mu][x_i - \mu])
$$

into _covariance_
 
$$
\gamma_{x,y} = \text{E}([x_i - \mu_x][y_i - \mu_y])
$$

We can estimate $\gamma_{x,y}$ from a sample as

$$
\text{Cov}(x,y) = \frac{1}{N-1}\sum_{i=1}^N{(x_i - m_x)(y_i - m_y)}
$$


## Graphical example of covariance

```{r, fig.align="center"}
# create dummy x set
xx <- runif(25, 0, 10)
yy <- 1 + 0.3*xx + rnorm(25,0,1.5)

par(mai=c(1,1,0,0), omi=c(0,0,0.5,1))
plot(xx, yy, pch=16, asp=1, las = 1,
     cex=1.2, cex.lab=1.2, col="black",
     xlab=expression(italic(x)),
     ylab=expression(italic(y)),
     main="")

```


## Graphical example of covariance

```{r, fig.align="center"}
par(mai=c(1,1,0,0), omi=c(0,0,0.5,1))
plot(xx, yy, pch=16, asp=1, las = 1,
     cex=1.2, cex.lab=1.2, col="black",
     xlab=expression(italic(x)),
     ylab=expression(italic(y)),
     main="")

# add mean lines	
abline(h=mean(yy), lty="dashed")
abline(v=mean(xx), lty="dashed")
# add labels for means
mtext(side=3, line=0.4, at=mean(xx), expression(italic(m[x])))
mtext(side=4, line=0.5, at=mean(yy), expression(italic(m[y])), las=1)

# add quadrant labels
mtext(side=3, line=0.4, at=(mean(xx)+par()$usr[1])/2, expression((italic(x[i])-italic(m[x])) < 0))
mtext(side=3, line=0.4, at=(mean(xx)+par()$usr[2])/2, expression((italic(x[i])-italic(m[x])) > 0))
mtext(side=4, line=0.5, at=(mean(yy)+par()$usr[3])/2, expression((italic(y[i])-italic(m[y])) < 0), las=1)
mtext(side=4, line=0.5, at=(mean(yy)+par()$usr[4])/2, expression((italic(y[i])-italic(m[y])) > 0), las=1)
```


## Graphical example of covariance

```{r, fig.align="center"}
par(mai=c(1,1,0,0), omi=c(0,0,0.5,1))
plot(xx, yy, pch=16, asp=1, las = 1,
     cex=1.2, cex.lab=1.2, col="gray",
     xlab=expression(italic(x)),
     ylab=expression(italic(y)),
     main="")

# add mean lines	
abline(h=mean(yy), lty="dashed")
abline(v=mean(xx), lty="dashed")
# add labels for means
mtext(side=3, line=0.4, at=mean(xx), expression(italic(m[x])))
mtext(side=4, line=0.5, at=mean(yy), expression(italic(m[y])), las=1)

# add quadrant labels
mtext(side=3, line=0.4, at=(mean(xx)+par()$usr[1])/2, expression((italic(x[i])-italic(m[x])) < 0))
mtext(side=3, line=0.4, at=(mean(xx)+par()$usr[2])/2, expression((italic(x[i])-italic(m[x])) > 0))
mtext(side=4, line=0.5, at=(mean(yy)+par()$usr[3])/2, expression((italic(y[i])-italic(m[y])) < 0), las=1)
mtext(side=4, line=0.5, at=(mean(yy)+par()$usr[4])/2, expression((italic(y[i])-italic(m[y])) > 0), las=1)

# get indices for data pairs with neg cov
negC <- (xx<mean(xx) & yy>mean(yy)) | (xx>mean(xx) & yy<mean(yy))

# overlay pos & neg cov values
points(xx[negC], yy[negC], pch="-", cex=2, col="darkred")
points(xx[!negC], yy[!negC], pch="+", cex=1.5, col="blue")
```


## Correlation

_Correlation_ is a dimensionless measure of the linear association between 2 variables, $x$ & $y$

It is simply the covariance standardized by the standard deviations

$$
\rho_{x,y} = \frac{\gamma_{x,y}}{\sigma_x \sigma_y}
$$

$$
-1 < \rho_{x,y} < 1
$$


## Correlation

_Correlation_ is a dimensionless measure of the linear association between 2 variables $x$ & $y$

It is simply the covariance standardized by the standard deviations

$$
\rho_{x,y} = \frac{\gamma_{x,y}}{\sigma_x \sigma_y}
$$

We can estimate $\rho_{x,y}$ from a sample as

$$
\text{Cor}(x,y) = \frac{\text{Cov}(x,y)}{s_x s_y}
$$


## Stationarity & the mean

Consider a single value, $x_t$


## Stationarity & the mean

Consider a single value, $x_t$

$\text{E}(x_t)$ is taken across an ensemble of _all_ possible time series


## Stationarity & the mean

```{r station_in_mean}
nn <- 200
tt <- 40
ww <- matrix(rnorm(nn*tt), tt, nn)

mm <- apply(ww, 1, mean)

par(mai = c(0.9,0.9,0.1,0.1), omi = c(0,0,0,0))

matplot(ww, type="l", lty="solid",  las = 1,
        ylab = expression(italic(x[t])), xlab = "Time",
        col = gray(0.5, 0.2))
points(rep(0,tt), pch = "-", col = "blue", cex=1.5)
```


## Stationarity & the mean

```{r ex_ts_plot_joint_dist, fig.cap="Our single realization is our estimate!"}

par(mai = c(0.9,0.9,0.1,0.1), omi = c(0,0,0,0))

matplot(ww, type="l", lty="solid",  las = 1,
        ylab = expression(italic(x[t])), xlab = "Time",
        col = gray(0.5, 0.2))
lines(ww[,1], col = "blue", lwd = 2)
```


## Stationarity & the mean

If $\text{E}(x_t)$ is constant across time, we say the time series is _stationary_ in the mean


## Stationarity of time series

_Stationarity_ is a convenient assumption that allows us to describe the statistical properties of a time series.

In general, a time series is said to be stationary if there is

1. no systematic change in the mean or variance  
2. no systematic trend  
3. no periodic variations or seasonality


## Identifying stationarity

```{r, fig.align="center"}
par(mfrow = c(2,2), mai = c(0.7,0.4,0.1,0.1))

plot.ts(arima.sim(model = list(ar = 0.3, sd = 0.1), n = 100),
        las = 1, ylab = "")
plot.ts(arima.sim(model = list(ar = 0.9, sd = 0.1), n = 100),
        las = 1, ylab = "")
plot.ts(arima.sim(model = list(ar = -0.9, sd = 0.1), n = 100),
        las = 1, ylab = "")
plot.ts(arima.sim(model = list(ma = 0.1, sd = 0.1), n = 100),
        las = 1, ylab = "")
```


## Identifying stationarity

Our eyes are really bad at identifying stationarity, so we will learn some tools to help us


## Autocovariance function (ACVF)

For stationary ts, we define the _autocovariance function_ ($\gamma_k$) as

$$
\gamma_k = \text{E}([x_t - \mu][x_{t+k} - \mu])
$$

which means that

$$
\gamma_0 = \text{E}([x_t - \mu][x_{t} - \mu]) = \sigma^2
$$


## Autocovariance function (ACVF)

For stationary ts, we define the _autocovariance function_ ($\gamma_k$) as

$$
\gamma_k = \text{E}([x_t - \mu][x_{t+k} - \mu])
$$

"Smooth" series have large ACVF for large $k$

"Choppy" series have ACVF near 0 for small $k$


## Autocovariance function (ACVF)

For stationary ts, we define the _autocovariance function_ ($\gamma_k$) as

$$
\gamma_k = \text{E}([x_t - \mu][x_{t+k} - \mu])
$$

We can estimate $\gamma_k$ from a sample as

$$
c_k = \frac{1}{N}\sum_{t=1}^{N-k}{(x_t - m)(x_{t+k} - m)}
$$


## Autocorrelation function (ACF)

The _autocorrelation function_ (ACF) is simply the ACVF normalized by the variance

$$
\rho_k = \frac{\gamma_k}{\sigma^2} = \frac{\gamma_k}{\gamma_0}
$$

The ACF measures the correlation of a time series against a time-shifted version of itself


## Autocorrelation function (ACF)

The _autocorrelation function_ (ACF) is simply the ACVF normalized by the variance

$$
\rho_k = \frac{\gamma_k}{\sigma^2} = \frac{\gamma_k}{\gamma_0}
$$

The ACF measures the correlation of a time series against a time-shifted version of itself

We can estimate ACF from a sample as

$$
r_k = \frac{c_k}{c_0}
$$


## Properties of the ACF

The ACF has several important properties:

- $-1 \leq r_k \leq 1$  

> - $r_k = r_{-k}$  

> - $r_k$ of periodic function is itself periodic  

> - $r_k$ for the sum of 2 independent variables is the sum of $r_k$ for each of them 


## The correlogram 

```{r, fig.cap="Graphical output for the ACF"}
par(mai=c(1,1,0,0), omi=c(0.1,0.1,0.1,0.1))
plot(NA, NA, type="n", xlim=c(0,15), ylim=c(-1,1),
     xlab="", xaxt="n", ylab="", las = 1)
abline(h=0)
axis(side=1, at=seq(15), labels=FALSE)
axis(side=1, at=seq(0,15,5))
mtext(expression(paste("Lag ", (italic(k)))), side=1, line=3, cex=1.2)
mtext(expression(paste("ACF ", (italic(r[k])))), side=2, line=3, cex=1.2)
```


## The correlogram 

```{r, fig.cap="The ACF at lag = 0 is always 1"}
par(mai=c(1,1,0,0), omi=c(0.1,0.1,0.1,0.1))
plot(NA, NA, type="n", xlim=c(0,15), ylim=c(-1,1),
     xlab="", xaxt="n", ylab="", las = 1)
abline(h=0)
axis(side=1, at=seq(15), labels=FALSE)
axis(side=1, at=seq(0,15,5))
mtext(expression(paste("Lag ", (italic(k)))), side=1, line=3, cex=1.2)
mtext(expression(paste("ACF ", (italic(r[k])))), side=2, line=3, cex=1.2)

lines(c(0,0), c(0,1), lwd=2, col="darkred")
text(x=1, y =1, expression(italic(r)[0] == 1), col="darkred")
```


## The correlogram 

```{r, fig.cap="Approximate confidence intervals"}
par(mai=c(1,1,0,0), omi=c(0.1,0.1,0.1,0.1))
plot(NA, NA, type="n", xlim=c(0,15), ylim=c(-1,1),
     xlab="", xaxt="n", ylab="", las = 1)
abline(h=0)
axis(side=1, at=seq(15), labels=FALSE)
axis(side=1, at=seq(0,15,5))
mtext(expression(paste("Lag ", (italic(k)))), side=1, line=3, cex=1.2)
mtext(expression(paste("ACF ", (italic(r[k])))), side=2, line=3, cex=1.2)

lines(c(0,0), c(0,1), lwd=2, col="darkred")
text(x=1, y =1, expression(italic(r)[0] == 1), col="darkred")

# add 95% CI's
nn <- 30
alpha <- 0.05
ts.SD <- qnorm(1-alpha/2, 0, 1)/sqrt(nn)
abline(h=-ts.SD, lty="dashed", col="blue")
text(x=14, y=-0.55, expression(-frac(italic(z)[1-frac(alpha,2)], sqrt(italic(n)))), col="blue")
abline(h=ts.SD, lty="dashed", col="blue")
text(x=14, y=0.55, expression(+frac(italic(z)[1-frac(alpha,2)], sqrt(italic(n)))), col="blue")
```


## ACF for deterministic forms

```{r}
## length of ts
nn <- 100

## trend only
par(mfrow=c(1,2), mai=c(1,1,0,0), omi=c(0.1,0.1,0.6,0.1))
tt <- seq(nn)
plot.ts(tt, ylab=expression(italic(x[t])))
acf(tt)
mtext("Linear trend {1,2,3,...,100}", outer=TRUE, line=1, cex=1.5)
```


## ACF for deterministic forms

```{r}
par(mfrow=c(1,2), mai=c(1,1,0,0), omi=c(0.1,0.1,0.6,0.1))
## compute the 2 predictor variables
tt <- sin(2*pi*seq(nn)/12)
plot.ts(tt, ylab=expression(italic(x[t])))
acf(tt)
mtext("Discrete (monthly) sine wave", outer=TRUE, line=1, cex=1.5)
```


## ACF for deterministic forms

```{r}
par(mfrow=c(1,2), mai=c(1,1,0,0), omi=c(0.1,0.1,0.6,0.1))
## compute the 2 predictor variables
tt <- sin(2*pi*seq(nn)/12) - seq(nn)/50
plot.ts(tt, ylab=expression(italic(x[t])))
acf(tt, lag.max=30)
mtext("Linear trend + seasonal effect", outer=TRUE, line=1, cex=1.5)
```


## ACF for deterministic forms

```{r}
par(mfrow=c(1,2), mai=c(1,1,0,0), omi=c(0.1,0.1,0.6,0.1))
# compute the 2 predictor variables
tt <- rep(floor(runif(nn/10,1,101)), times=10)
plot.ts(tt, ylab=expression(italic(x[t])))
acf(tt)
mtext("Sequence of 10 random numbers repeated 10 times", outer=TRUE, line=1, cex=1.5)
```


## Induced autocorrelation

Recall the transitive property, whereby

If $A = B$ and $B = C$, then $A = C$


## Induced autocorrelation

Recall the transitive property, whereby

If $A = B$ and $B = C$, then $A = C$

which suggests that

If $x \propto y$ and $y \propto z$, then $x \propto z$


## Induced autocorrelation

Recall the transitive property, whereby

If $A = B$ and $B = C$, then $A = C$

which suggests that

If $x \propto y$ and $y \propto z$, then $x \propto z$

and thus

If $x_t \propto x_{t+1}$ and $x_{t+1} \propto x_{t+2}$, then $x_t \propto x_{t+2}$


## Partial autocorrelation funcion (PACF)

The _partial autocorrelation function_ ($\phi_k$) measures the correlation between a series $x_t$ and $x_{t+k}$ with the linear dependence of $\{x_{t-1},x_{t-2},\dots,x_{t-k-1}\}$ removed


## Partial autocorrelation funcion (PACF)

The _partial autocorrelation function_ ($\phi_k$) measures the correlation between a series $x_t$ and $x_{t+k}$ with the linear dependence of $\{x_{t-1},x_{t-2},\dots,x_{t-k-1}\}$ removed


We can estimate $\phi_k$ from a sample as

$$
\phi_k =
    \begin{cases}
      \text{Cor}(x_1,x_0) = \rho_1 & \text{if } k = 1 \\
      \text{Cor}(x_k-x_k^{k-1}, x_0-x_0^{k-1}) & \text{if } k \geq 2
    \end{cases}
$$


$$
x_k^{k-1} = \beta_1 x_{k-1} + \beta_2 x_{k-2} + \dots + \beta_{k-1} x_1
$$

$$
x_0^{k-1} = \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_{k-1} x_{k-1}
$$


## Lake Washington phytoplankton

```{r lwa_phyto_ts}
library(MARSS)
data(lakeWAplankton)
lwa <- lakeWAplanktonTrans
lwa <- lwa[lwa[,"Year"] >= 1975,]
lwa <- ts(lwa, start = c(1975,1), freq = 12)
par(mai=c(1,1,0,0), omi=c(0.1,0.1,0.1,0.1))
plot.ts(lwa[,"Cryptomonas"], ylab=expression(log(italic(Cryptomonus))))
```


## Lake Washington phytoplankton

```{r lwa_phyto_acf, fig.cap = "Autocorrelation"}
par(mai=c(1,1,0,0), omi=c(0.1,0.1,0.1,0.1))
acf(lwa[,"Cryptomonas"], na.action = na.pass)
```


## Lake Washington phytoplankton

```{r lwa_phyto_pacf, fig.cap = "Partial autocorrelation"}
par(mai=c(1,1,0,0), omi=c(0.1,0.1,0.1,0.1))
pacf(lwa[,"Cryptomonas"], na.action = na.pass)
```


## ACF & PACF in model selection

The ACF & PACF will be _very_ useful for identifying the orders of ARMA models



## Cross-covariance function (CCVF)

Often we want to look for relationships between 2 different time series

We can extend the notion of covariance to _cross-covariance_


## Cross-covariance function (CCVF)

Often we want to look for relationships between 2 different time series

We can extend the notion of covariance to _cross-covariance_

We can estimate $g^{x,y}_k$ from a sample as

$$
g^{x,y}_k = \frac{1}{N}\sum_{t=1}^{N-k}{(x_t - m_x)(y_{t+k} - m_y)}
$$


## Cross-correlation function (CCF)

The cross-correlation function is the CCVF normalized by the standard deviations of x & y

$$
r^{x,y}_k = \frac{g^{x,y}_k}{s_x s_y}
$$

Just as with other measures of correlation

$$
-1 \leq r^{x,y}_k \leq 1
$$


## Example of cross-correlation

```{r, fig.align="center"}
## get the matching years of sunspot data
suns <- ts.intersect(lynx,sunspot.year)[,"sunspot.year"]
## get the matching lynx data
lynx <- ts.intersect(lynx,sunspot.year)[,"lynx"]

layout(mat = matrix(c(1,1,2,2,0,3,3,0), 4, 2))

par(mai=c(0.1,0.6,0.1,0.3), omi=c(0.5,0,0,0))
plot.ts(suns, main="", ylab="Sunspot activity",
        xaxt="n", xlab="", cex.lab = 2)

# par(mai=c(0.6,0.5,0.1,0.1), omi=c(0,0,0,0))
plot.ts(lynx, ylab="Number of trapped lynx")

# par(mai=c(0.6,0.5,0,0.1))
ccf(log(lynx), log(suns), ylab="Cross-correlation", main="")
```


## {.flexbox .vcenter}

<font size="10">SOME SIMPLE MODELS</font>


## White noise (WN)

A time series $\{w_t\}$ is discrete white noise if its values are

1. independent  

2. identically distributed with a mean of zero


## White noise (WN)

A time series $\{w_t\}$ is discrete white noise if its values are

1. independent  

2. identically distributed with a mean of zero

Note that distributional form for $\{w_t\}$ is flexible


## White noise (WN)

```{r, fig.cap="$w_t = 2e_t - 1; e_t \\sim \\text{Bernoulli}(0.5)$"}
par(mfrow = c(1,2), mai = c(1.5,0.9,0.1,0.1), omi = c(0,0,0,0))
tt <- rbinom(100, 1, 0.5) * 2 - 1
plot.ts(tt, ylab = expression(italic(w[t])))
acf(tt)
```


## Gaussian white noise

We often assume so-called _Gaussian white noise_, whereby

$$
w_t \sim \text{N}(0,\sigma^2)
$$


## Gaussian white noise

We often assume so-called _Gaussian white noise_, whereby

$$
w_t \sim \text{N}(0,\sigma^2)
$$

and the following apply as well

&nbsp; &nbsp; autocovariance:&nbsp; $\gamma_k =
    \begin{cases}
      \sigma^2 & \text{if } k = 0 \\
      0 & \text{if } k \geq 1
    \end{cases}$

&nbsp; &nbsp; autocorrelation: &nbsp; $\rho_k =
    \begin{cases}
      1 & \text{if } k = 0 \\
      0 & \text{if } k \geq 1
    \end{cases}$


## Gaussian white noise

```{r ex_gaussian_wn, fig.cap="$w_t \\sim \\text{N}(0,1)$"}
par(mfrow = c(1,2), mai = c(1.5,0.9,0.1,0.1), omi = c(0,0,0,0))
tt <- rnorm(100)
plot.ts(tt, ylab = expression(italic(w[t])))
acf(tt)
```


## Random walk (RW)

A time series $\{x_t\}$ is a random walk if

1. $x_t = x_{t-1} + w_t$  

2. $w_t$ is white noise


## Random walk (RW)

The following apply to random walks

&nbsp; &nbsp; mean: &nbsp; $\mu_x = 0$

&nbsp; &nbsp; autocovariance: &nbsp; $\gamma_k(t) = t \sigma^2$

&nbsp; &nbsp; autocorrelation: &nbsp; $\rho_k(t) = \frac{t \sigma^2}{\sqrt{t \sigma^2(t + k) \sigma^2}}$


## Random walk (RW)

The following apply to random walks

&nbsp; &nbsp; mean: &nbsp; $\mu_x = 0$

&nbsp; &nbsp; autocovariance: &nbsp; $\gamma_k(t) = t \sigma^2$

&nbsp; &nbsp; autocorrelation: &nbsp; $\rho_k(t) = \frac{t \sigma^2}{\sqrt{t \sigma^2(t + k) \sigma^2}}$

_Note_: Random walks are not stationary


## Random walk (RW)

```{r ex_rw, fig.cap="$x_t = x_{t-1} + w_t; w_t \\sim \\text{N}(0,1)$"}
par(mfrow = c(1,2), mai = c(1.5,0.9,0.1,0.1), omi = c(0,0,0,0))
tt <- cumsum(rnorm(100))
plot.ts(tt, ylab = expression(italic(x[t])))
acf(tt)
```


## {.flexbox .vcenter}

<font size="10">SOME IMPORTANT OPERATORS</font>


## The backshift shift operator

The _backshift shift operator_ ($\mathbf{B}$) is an important function in time series analysis, which we define as

$$
\mathbf{B} x_t = x_{t-1}
$$

or more generally as

$$
\mathbf{B}^k x_t = x_{t-k}
$$

## The backshift shift operator

For example, a random walk with

$$
x_t = x_{t-1} + w_t
$$

can be written as

$$
\begin{align}
  x_t &= \mathbf{B} x_t + w_t \\
  x_t - \mathbf{B} x_t &= w_t \\
  (1 - \mathbf{B}) x_t &= w_t \\
  x_t &= (1 - \mathbf{B})^{-1} w_t
\end{align}
$$


## The difference operator

The _difference operator_ ($\nabla$) is another important function in time series analysis, which we define as

$$
\nabla x_t = x_t - x_{t-1}
$$


## The difference operator

The _difference operator_ ($\nabla$) is another important function in time series analysis, which we define as

$$
\nabla x_t = x_t - x_{t-1}
$$

For example, first-differencing a random walk yields white noise

$$
\begin{align}
  \nabla x_t &= x_{t-1} + w_t \\
  x_t - x_{t-1} &= x_{t-1} + w_t - x_{t-1}\\
  x_t - x_{t-1} &= w_t\\
\end{align}
$$


## The difference operator

The difference operator and the backshift operator are related

$$
\nabla^k = (1 - \mathbf{B})^k
$$


## The difference operator

The difference operator and the backshift operator are related

$$
\nabla^k = (1 - \mathbf{B})^k
$$

For example

$$
\begin{align}
  \nabla x_t &= (1 - \mathbf{B})x_t \\
  x_t - x_{t-1} &= x_t - \mathbf{B} x_t \\
  x_t - x_{t-1} &= x_t - x_{t-1}
\end{align}
$$


## Differencing to remove a trend

Differencing is a simple means for removing a trend

The 1st-difference removes a linear trend; a 2nd-difference would remove a quadratic trend, etc.


## Differencing to remove a trend

```{r diff_linear, fig.align="center"}
## create biased RW
rr <- ww <- rnorm(50)
for(t in 2:50) {
  rr[t] <- 0.3 + rr[t-1] + ww[t]
}

par(mfrow = c(2,1), mai = c(0.5,0.8,0.1,0), omi=c(0,0,0,0))

## raw data
plot.ts(rr, las = 1,
        ylab = expression(italic(x[t])))
## first difference
plot.ts(diff(rr), las = 1,
        ylab = expression(nabla~italic(x[t])))
```


## Differencing to remove seasonality

Differencing is a simple means for removing a seasonal effect

Using a 1st-difference with $k = period$ removes both trend & seasonal effects


## Differencing to remove seasonality

```{r diff_season, fig.align="center"}
par(mfrow = c(2,1), mai = c(0.5,0.8,0.1,0), omi=c(0,0,0,0))

## raw data
plot.ts(lwa[,"Temp"], las = 1,
        ylab = "Temperature")
## first difference
plot.ts(diff(lwa[,"Temp"], 12), las = 1,
        ylab = expression(nabla~Temperature))
```


## Topics for today

Characteristics of time series

* Expectation, mean & variance
* Covariance & correlation
* Stationarity
* Autocovariance & autocorrelation
* Correlograms

White noise

Random walks

Backshift & difference operators

