---
title: "Stationarity & simple models"
subtitle: "FISH 507 â€“ Applied Time Series Analysis"
author: "Mark Scheuerell"
date: "10 Jan 2019"
output:
  ioslides_presentation:
    css: lecture_slides.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Topics for today

### Characteristics of time series (ts)
* Expectation, mean & variance
* Covariance & correlation
* Stationarity
* Autocovariance & autocorrelation
* Correlograms
* White noise
* Random walks
* Backshift & difference operators


## Expectation & the mean

The expectation (E) of a variable is its mean value in the population

$\text{E}(x) \equiv$ mean of $x = \mu$

Can estimate $\mu$ from a sample as

$$
\text{E}(x) = m = \frac{\sum_{i=1}^N{x_i}}{N}
$$


## Variance

$\text{E}([x - \mu]^2) \equiv$ mean deviations of $x$ about $\mu$

$\text{E}([x - \mu]^2) \equiv$ variance of $x = \sigma^2$

Can estimate $\sigma^2$ from a sample as

$$
\text{Var}(x) = s^2 = \frac{1}{N-1}\sum_{i=1}^N{(x_i - m)^2}
$$


## Covariance

If we have two variables, $x$ and $y$, we can generalize variance

$$
\sigma^2 = \text{E}([x_i - \mu]^2) = \text{E}([x_i - \mu][x_i - \mu])
$$

into _covariance_
 
$$
\gamma_{x,y} = \text{E}([x_i - \mu_x][y_i - \mu_y])
$$

Can estimate $\gamma_{x,y}$ from a sample as

$$
\text{Cov}(x,y) = \frac{1}{N-1}\sum_{i=1}^N{(x_i - m_x)(y_i - m_y)}
$$


## Graphical example of covariance

```{r}
# create dummy x set
xx <- runif(25, 0, 10)
yy <- 1 + 0.3*xx + rnorm(25,0,1.5)

par(mai=c(1,1,0,0), omi=c(0,0,0.5,1))
plot(xx, yy, pch=16, asp=1, las = 1,
     cex=1.2, cex.lab=1.2, col="black",
     xlab=expression(italic(x)),
     ylab=expression(italic(y)),
     main="")

```


## Graphical example of covariance

```{r}
par(mai=c(1,1,0,0), omi=c(0,0,0.5,1))
plot(xx, yy, pch=16, asp=1, las = 1,
     cex=1.2, cex.lab=1.2, col="black",
     xlab=expression(italic(x)),
     ylab=expression(italic(y)),
     main="")

# add mean lines	
abline(h=mean(yy), lty="dashed")
abline(v=mean(xx), lty="dashed")
# add labels for means
mtext(side=3, line=0.4, at=mean(xx), expression(italic(m[x])))
mtext(side=4, line=0.5, at=mean(yy), expression(italic(m[y])), las=1)

# add quadrant labels
mtext(side=3, line=0.4, at=(mean(xx)+par()$usr[1])/2, expression((italic(x[i])-italic(m[x])) < 0))
mtext(side=3, line=0.4, at=(mean(xx)+par()$usr[2])/2, expression((italic(x[i])-italic(m[x])) > 0))
mtext(side=4, line=0.5, at=(mean(yy)+par()$usr[3])/2, expression((italic(y[i])-italic(m[y])) < 0), las=1)
mtext(side=4, line=0.5, at=(mean(yy)+par()$usr[4])/2, expression((italic(y[i])-italic(m[y])) > 0), las=1)
```


## Graphical example of covariance

```{r}
par(mai=c(1,1,0,0), omi=c(0,0,0.5,1))
plot(xx, yy, pch=16, asp=1, las = 1,
     cex=1.2, cex.lab=1.2, col="gray",
     xlab=expression(italic(x)),
     ylab=expression(italic(y)),
     main="")

# add mean lines	
abline(h=mean(yy), lty="dashed")
abline(v=mean(xx), lty="dashed")
# add labels for means
mtext(side=3, line=0.4, at=mean(xx), expression(italic(m[x])))
mtext(side=4, line=0.5, at=mean(yy), expression(italic(m[y])), las=1)

# add quadrant labels
mtext(side=3, line=0.4, at=(mean(xx)+par()$usr[1])/2, expression((italic(x[i])-italic(m[x])) < 0))
mtext(side=3, line=0.4, at=(mean(xx)+par()$usr[2])/2, expression((italic(x[i])-italic(m[x])) > 0))
mtext(side=4, line=0.5, at=(mean(yy)+par()$usr[3])/2, expression((italic(y[i])-italic(m[y])) < 0), las=1)
mtext(side=4, line=0.5, at=(mean(yy)+par()$usr[4])/2, expression((italic(y[i])-italic(m[y])) > 0), las=1)

# get indices for data pairs with neg cov
negC <- (xx<mean(xx) & yy>mean(yy)) | (xx>mean(xx) & yy<mean(yy))

# overlay pos & neg cov values
points(xx[negC], yy[negC], pch="-", cex=2, col="darkred")
points(xx[!negC], yy[!negC], pch="+", cex=1.5, col="blue")
```


## Correlation

_Correlation_ is a dimensionless measure of the linear association between 2 variables $x$ & $y$

It is simply the covariance standardized by the standard deviations

$$
\rho_{x,y} = \frac{\gamma_{x,y}}{\sigma_x \sigma_y}
$$

$$
-1 < \rho_{x,y} < 1
$$


## Correlation

_Correlation_ is a dimensionless measure of the linear association between 2 variables $x$ & $y$

It is simply the covariance standardized by the standard deviations

$$
\rho_{x,y} = \frac{\gamma_{x,y}}{\sigma_x \sigma_y}
$$

We can estimate $\rho_{x,y}$ from a sample as

$$
\text{Cor}(x,y) = \frac{\text{Cov}(x,y)}{s_x s_y}
$$


## Stationarity of time series






