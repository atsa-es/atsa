---
title: "Hidden Markov Models and Bayesian MARSS models"
author: "Eli Holmes, Eric Ward"
date: "17 February 2021"
output:
  html_document:
    theme: cosmo
    highlight: textmate
    toc: true
    toc_float: true
    toc_depth: 3
---

```{r dlm-setup, include=FALSE, purl=FALSE}
#in case you forget to add a chunk label
knitr::opts_knit$set(unnamed.chunk.label = "hw6-")
```

<br>

# Hidden Markov Models {#sec-hmm}

For the second part of the homework, we'll use data from the Pacific Decadal Oscillation (PDO) to ask questions about identifying regimes. This dataset can be accessed via the `rpdo` package. First, let's grab the data. 

```{r read-data}
library(dplyr)
#install.packages("rsoi")
pdo <- rsoi::download_pdo()
pdo$water_year = pdo$Year
pdo$water_year[which(pdo$Month%in%c("Oct","Nov","Dec"))] = pdo$water_year[which(pdo$Month%in%c("Oct","Nov","Dec"))] + 1
pdo = dplyr::group_by(pdo, water_year) %>%
  dplyr::summarize(winter_pdo = mean(PDO[which(Month %in% c("Oct","Nov","Dec","Jan","Feb"))])) %>% 
  dplyr::select(winter_pdo,water_year) %>% 
  dplyr::rename(year=water_year)
```

1. Identifying regimes using Hidden Markov Models (HMMs)

    a. Start by fitting a 2-state HMM to the annual indices of winter PDO. You're welcome to use any package / method you like ('depmixS4' would be a good choice if you're unsure). Assume Gaussian errors. 
    
```{r}
library("depmixS4")
mod <- depmix(response = winter_pdo ~ 1, data = pdo, nstates = 2, trstart = runif(4))
fitmod <- fit(mod)
```

```{r}
prstates = apply(posterior(fitmod)[,c("S1","S2")], 
  1, which.max)
plot(prstates, type="b", xlab="Time", ylab="State")
```
    
    b. Try to fit the model 10-20 times. Does the likelihood seem reasonably stable?
    
```{r}
ll <- rep(NA,20)
for(i in 1:20){
mod <- depmix(response = winter_pdo ~ 1, data = pdo, nstates = 2)
fitmod <- fit(mod)
ll[i] <- logLik(fitmod)
}
```

```{r}
hist(ll, main="logLik")
```
    
    d. What is the transition matrix for the best model? What are the persistence probabilities (e.g. probabilities of being in the same state)?
    
```{r}
summary(fitmod, which="transition")
```

d. Using slide 15 of the HMM lecture as a template, write out your fitted HMM model as equations.

    e. Plot the expected state versus year. See slide 50 of the HMM lecture for an example.
    
```{r}
prstates = apply(posterior(fitmod)[,c("S1","S2")], 
  1, which.max)
plot(prstates, type="b", xlab="Time", ylab="State")
```

    e. Plot the posterior probability of being in the various states from your best model (e.g. probability of being in state 1 over time)
    
```{r}
plot(pdo$year, posterior(fitmod)[,c("S1")], type="l")
lines(pdo$year, posterior(fitmod)[,c("S2")], col="red", lty=2)
```
    
    f. Plot the time series of predicted values from the model
    
```{r}
mu <- summary(fitmod)[,1]
prstates = apply(posterior(fitmod)[,c("S1","S2")], 
  1, which.max)
pred <- data.frame(year=pdo$year, fit= mu[prstates])
plot(pdo$year, pdo$winter_pdo, ylab="pdo", xlab="year")
lines(pdo$year, pred$fit, lwd=2)
mu <- summary(fitmod3)[,1]
prstates = apply(posterior(fitmod3)[,c("S1","S2","S3")], 
  1, which.max)
pred <- data.frame(year=pdo$year, fit= mu[prstates])
lines(pdo$year, pred$fit, col="blue")
abline(h=0, lty=3)
```

    c. Change the model to a 1-state, 3-state model and a 4-state model. Using AIC as a model selection metric, which model performs better (lower AIC): 1, 2, 3, or 4 state?
    
```{r}
df <- data.frame(model=paste("State", 1:4), AIC=NA)
for(i in 1:4){
mod <- depmix(response = winter_pdo ~ 1, data = pdo, nstates = i)
fit <- fit(mod)
df$AIC[i] <- AIC(fit)
}
knitr::kable(df)
```


**Some ideas for OPTIONAL extra analyses**


    

* If you include time varying parameters (e.g. year) in the means of each state, or state transition probabilities, does the model do any better?

* Run diagnostics on the best model. Any problems?

* Compare the transition matrices for fits with different random starting conditions. Are the transition matrices stable?

* What is the long-run probability that the PDO is in state 1 versus state 2? You can calculate this from the transition matrix. There is an analytical solution for this (a bit of googling will find it). Or you can run a `for` loop to find it. Let $p_1$ be the probability that PDO is in state 1 and $p_2$ be the probability that PDO is in state 2. The if $P$ is the transition matrix,
$$\begin{bmatrix}p_1&p_2\end{bmatrix}_n = begin{bmatrix}p_1&p_2\end{bmatrix}_{n-1} P$$
Start with $p_1=1$ and $p_2=0$, say. Run a `for` loop until 
$$\begin{bmatrix}p_1&p_2\end{bmatrix}_n = begin{bmatrix}p_1&p_2\end{bmatrix}_{n-1}$$
That $\begin{bmatrix}p_1&p_2\end{bmatrix}_n$ is the stationary distribution.
    
<br>


2. Bayesian MARSS modelling {#sec-bayes}

Building off what we did for lab last week, we're going to work with the NEON data from Barco Lake, doing simultaneous modeling of oxygen and temperature. For this exercise, we'll only work with the data from the first four months of 2020 (where there was consistent data for each of the responses).

```{r read-data-neon}
data(neon_barc, package = "atsalibrary")
data <- neon_barc

# we're going to just work with data from 2020
data = dplyr::mutate(data,
        year = lubridate::year(data$date),
        yday = lubridate::yday(data$date),
        month = lubridate::month(data$date)) %>%
  dplyr::filter(year==2020, month < 5)
```

If you were to try to fit the temperature and oxygen data using MARSS, you'll notice that it's difficult to get the model to converge. Why? In the absence of covariates, it seems like there's some support for including moving average components. For oxygen in particular, we can see this with the PACF plot,

```{r pacf, echo=FALSE}
pacf((data$oxygen),na.action = na.pass)
```

So instead of fitting this model using a Bayesian version of MARSS, we're going to fit this using a moving average model. We've made a few other changes to the model that we can't do in a Bayesian setting. Most importantly we'll model the MA deviations as a random walk, generated from a multivariate Student-t distribution (if you examine the first differenced temperature or oxygen series, you'll see the data support a heavy tailed distribution). 

Here's the code, that will create a file called "model.stan" in your working directory,

```{r create-model}
model <- cat("data {
  int<lower=0> n; // size of trainging set (with NAs)
  int<lower=0> n_o2; // number of observations
  int o2_x[n_o2];
  real o2_y[n_o2];
  real o2_z[n_o2];  
  real o2_sd[n_o2];
  int<lower=0> n_temp; // number of observations
  int temp_x[n_temp];
  real temp_y[n_temp];
  real temp_sd[n_temp];  
  int n_forecast;
  int n_lag_o2;
  int n_lag_temp;
}
transformed data {
  vector[2] zeros;
  matrix[2,2] identity;
  int lag_o2;
  int lag_temp;  
  zeros = rep_vector(0, 2);
  identity = diag_matrix(rep_vector(1.0,2));

  lag_o2 = 1;
  lag_temp = 1;
  if(n_lag_o2 > 1) lag_o2 = 1;
  if(n_lag_temp > 1) lag_temp = 1;  
}
parameters {
  vector[2] devs[n+n_forecast-1];
  cov_matrix[2] Sigma;
  real<lower=0> o2_est_sd;
  real<lower=0> temp_est_sd;
  real o2_x0[lag_o2]; // initial conditions
  real temp_x0[lag_temp]; // initial conditions
  real u_temp;
  real u_o2;
}
transformed parameters {
  vector[n+n_forecast] o2_pred;
  vector[n_forecast] o2_forecast;
  vector[n+n_forecast] temp_pred;
  vector[n_forecast] temp_forecast;  
  real rho;
  // calc correlation
  rho = Sigma[1,2]/(sqrt(Sigma[1,1])*sqrt(Sigma[2,2]));
  // predictions for first states
  for(t in 1:lag_o2) {
    o2_pred[t] = o2_x0[t];
  }
  for(t in 1:lag_o2) {
    temp_pred[t] = temp_x0[t];
  }  

  for(i in (1+lag_o2):(n+n_forecast)) {
    o2_pred[i] = o2_pred[i-1] + devs[i-1,1];
  }
  for(i in (1+lag_temp):(n+n_forecast)) {
    temp_pred[i] = temp_pred[i-1] + devs[i-1,2];
  }

  // this is redundant but easier to work with output -- creates object o2_forecast
  // containing forecast n_forecast time steps ahead
  for(t in 1:n_forecast) {
    o2_forecast[t] = o2_pred[n_o2+t];
    temp_forecast[t] = temp_pred[n_temp+t];
  }
    
}
model {
  // initial conditions, centered on mean
  o2_x0 ~ normal(7,3);
  temp_x0 ~ normal(22,3);
  o2_est_sd ~ student_t(3,0,2);
  temp_est_sd ~ student_t(3,0,2);
  // df parameter
  devs[1] ~ multi_student_t(3, zeros, Sigma);
  for(i in 2:(n+n_forecast-1)) {
    devs[i] ~ multi_student_t(3, devs[i-1], Sigma);
  }

  // likelihood
  for(t in 1:n_o2) {
    o2_y[t] ~ normal(o2_pred[o2_x[t]], o2_est_sd);
  }
  for(t in 1:n_temp) {
    temp_y[t] ~ normal(temp_pred[temp_x[t]], temp_est_sd);
  }
} 
", file="model.stan")
```

Next we have to create the data. We'll use a similar version of the function that we gave you last week,

```{r}
data$indx <- seq(1, nrow(data)) # dimension of data
n_forecast <- 7 # forecast horizon
n_lag_o2 <- 0 # lag for AR component
n_lag_temp <- 0 # lag for AR component
last_obs <- nrow(data)

create_stan_data <- function(data, last_obs, n_forecast, n_lag_o2, 
    n_lag_temp) {
    # create test data
    o2_test <- dplyr::filter(data, indx %in% seq(last_obs + 1, 
        (last_obs + n_forecast)))
    temp_test <- dplyr::filter(data, indx %in% seq(last_obs + 
        1, (last_obs + n_forecast)))
    
    o2_train <- dplyr::filter(data, indx <= last_obs, !is.na(oxygen), !is.na(depth_oxygen))
    o2_x <- o2_train$indx
    o2_y <- o2_train$oxygen
    o2_z <- o2_train$depth_oxygen
    o2_sd <- o2_train$oxygen_sd
    n_o2 <- nrow(o2_train)
    
    temp_train <- dplyr::filter(data, indx <= last_obs, !is.na(temperature))
    temp_x <- temp_train$indx
    temp_y <- temp_train$temperature
    temp_sd <- temp_train$temperature_sd
    n_temp <- nrow(temp_train)
    
    stan_data <- list(n = last_obs, n_lag_o2 = n_lag_o2, n_lag_temp = n_lag_temp, 
        n_forecast = n_forecast, n_o2 = n_o2, o2_x = o2_x, o2_y = o2_y, o2_z = o2_z,
        o2_sd = o2_sd, n_temp = n_temp, temp_x = temp_x, temp_y = temp_y, 
        temp_sd = temp_sd)
    
    return(list(o2_train = o2_train, temp_train = temp_train, 
        stan_data = stan_data, o2_test = o2_test, temp_test = temp_test))
}

stan_data <- create_stan_data(data, last_obs, n_forecast, n_lag_o2, 
    n_lag_temp)
```


And finally, we can fit the model. The `pars` here includes a list of the initial conditions (`o2_x0`,`temp_x0`), the covariance matrix of the process deviations (`Sigma`), the correlation between oxygen and temperature deviations (as a derived parameter `rho`) and the predicted values of temperature (`temp_pred`) and oxygen (`o2_pred`). 

You'll probably want to just do this once (it may take ~ 15 minutes) and save the object 
```{r eval=FALSE}
fit <- stan(file="model.stan", data=stan_data$stan_data, 
            chains=3, 
            iter=3000,
            pars=c("o2_x0","temp_x0","Sigma","rho","temp_pred","o2_pred"))
pars = extract(fit)
```


    a. Using the interactive 'shinystan', does it appear the model converged? You can be qualitative and brief, with a focus just on the initial conditions and variance parameters. 
    
    b. Similarly, are any of the R-hat values for the parameters from #1 > 1.1? You can see this in the table created by summary(fit)$summary
    
    c. Make a posterior histogram, or density plot, of the derived correlation between oxygen and temperature. Is there support for them being negatively correlated, not correlated, or positively correlated?
    
    d. Make predictions of the estimated temperature and oxygen time series from the model, along with 95% CIs on each. Because of differing scales, these can be on different plots





