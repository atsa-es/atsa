---
title: "Hidden Markov Models and Bayesian MARSS models"
author: "Eli Holmes, Eric Ward"
date: "17 February 2021"
output:
  html_document:
    theme: cosmo
    highlight: textmate
    toc: true
    toc_float: true
    toc_depth: 3
---

```{r dlm-setup, include=FALSE, purl=FALSE}
#in case you forget to add a chunk label
knitr::opts_knit$set(unnamed.chunk.label = "hw6-")
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE)
```

<br>

# Part 1. Hidden Markov Models

For Part 1 of the homework, we'll use data from the Pacific Decadal Oscillation (PDO) to ask questions about identifying regimes. This dataset can be accessed via the `rpdo` package. First, let's grab the data. 

```{r read-data}
library(dplyr)
#install.packages("rsoi")
pdo <- rsoi::download_pdo()
```
We will look at the winter PDO only. We need to shift the year for Oct-Dec by +1 since Oct-Feb spans 2 calendar years. Then we need to remove the first year since that would only have Jan-Feb.
```{r read-data2}
pdo$Year[which(pdo$Month%in%c("Oct","Nov","Dec"))] <- pdo$Year[which(pdo$Month%in%c("Oct","Nov","Dec"))] + 1
pdo <- dplyr::group_by(pdo, Year) %>%
  dplyr::summarize(winter_pdo = mean(PDO[which(Month %in% c("Oct","Nov","Dec","Jan","Feb"))])) %>% 
  dplyr::select(winter_pdo, Year)
# The first year will be missing Oct-Dec
pdo <- pdo[-1,]
```

1. Fit a 2-state HMM to the annual indices of winter PDO. Assume Gaussian errors (default). 
    
```{r}
library("depmixS4")
mod <- depmix(response = winter_pdo ~ 1, data = pdo, nstates = 2, trstart = runif(4))
fitmod <- fit(mod)
```

```{r}
prstates <- apply(posterior(fitmod)[,c("S1","S2")], 
  1, which.max)
plot(prstates, type="b", xlab="Time", ylab="State")
```
    
2. Try fitting the model 10-20 times. Does the likelihood seem reasonably stable? (Note `logLik()` gets you the log-likelihood from model fits in R).
    
```{r}
ll <- rep(NA,20)
for(i in 1:20){
mod <- depmix(response = winter_pdo ~ 1, data = pdo, nstates = 2)
fitmod <- fit(mod)
ll[i] <- logLik(fitmod)
}
```

Looks good.
```{r}
hist(ll, main="logLik")
```
    
3. What is the transition matrix for the best model? What are the persistence probabilities (e.g. probabilities of staying in the same state from step $t$ to $t+1$)?
    
```{r}
summary(fitmod, which="transition")
```

Make a matrix.
```{r}
GAM <- matrix(getpars(fitmod)[3:6],2,2, byrow=TRUE)
GAM
```
Persistence probabilites are `r GAM[1,1]` for staying in state 1 andd `r GAM[2,2]` for staying state 2.

4. Using slide 15 of the HMM lecture as a template and the matrix in Q3, write out your fitted HMM model as equations.

$$
P[x_{t+1}=j|x_t=i] = \gamma_{i,j}
$$

The $\gamma_{i,j}$ matrix is $\Gamma$, where the rows are *from* so $i$ and the columns are *to* so $j$. So looks just like the matrix in Question 3.
$$
\Gamma = \begin{bmatrix}\gamma_{1,1}&\gamma_{1,2}\\
\gamma_{2,1}&\gamma_{2,2}\end{bmatrix} = \begin{bmatrix}0.848&0.152\\0.116&0.884\end{bmatrix}
$$

5. Plot the predicted values versus year. See slide 50 of the HMM lecture for an example.
    
```{r}
prstates = apply(posterior(fitmod)[,c("S1","S2")], 
  1, which.max)
plot(prstates, type="b", xlab="Time", ylab="State")
```

6. Plot the posterior probability of being in the various states from your best model (e.g. probability of being in state 1 over time)   
    
```{r}
plot(pdo$Year, posterior(fitmod)[,c("S1")], type="l")
lines(pdo$Year, posterior(fitmod)[,c("S2")], col="red", lty=2)
```
    
7. What is the long-run probability that the PDO is in state 1 versus state 2? You can calculate this from the transition matrix. There is an analytical solution for this (a bit of googling will find it). Or you can run a `for` loop to find it. Let $p_1$ be the probability that PDO is in state 1 and $p_2$ be the probability that PDO is in state 2. Note $p_1 + p_2 = 1$. If $P$ is the transition matrix (in Q3),

$$\begin{bmatrix}p_1&p_2\end{bmatrix}_n = \begin{bmatrix}p_1&p_2\end{bmatrix}_{n-1} P$$
Note this is a 1x2 matrix times a 2x2 matrix on the right. Start with $p_1=1$ and $p_2=0$, say. Run a `for` loop until 

$$\begin{bmatrix}p_1&p_2\end{bmatrix}_n \approx \begin{bmatrix}p_1&p_2\end{bmatrix}_{n-1}$$

```{r}
x.stationary <- matrix(c(0,1), 1, 2)
for(i in 1:1000) x.stationary <- x.stationary %*% GAM
x.stationary
```
So about 43.4% of the time, the PDO will be in state 1.

To get the analytical solution, find the stationary distribution of a Markov chain. Watch [this youtube video](https://www.youtube.com/watch?v=j93286JaPY8) for example. You solve this linear equation:
$$
\begin{bmatrix}p_1\\p_2\end{bmatrix} = P^\top \begin{bmatrix}p_1\\p_2\end{bmatrix}
$$
with the constraint that $p_1 + p_2 = 1$. One of the constraints from our transition equation is extra, so we remove that. That's the `-1` in the code.

```{r}
b <- matrix(c(0,1), ncol=1)
A <- rbind(t(GAM)-diag(1,2), 1)
solve(A[-1,], b)
```

**Optional Analyses**

* Change the model to a 1-state, 3-state model and a 4-state model. Using AIC as a model selection metric, which model performs better (lower AIC): 1, 2, 3, or 4 state?

2-state is best via AIC.

```{r}
df <- data.frame(model=paste("State", 1:4), AIC=NA)
for(i in 1:4){
mod <- depmix(response = winter_pdo ~ 1, data = pdo, nstates = i)
fit <- fit(mod)
df$AIC[i] <- AIC(fit)
}
knitr::kable(df)
```

* Compare the transition matrices for fits with different random starting conditions. Are the transition matrices stable?

Looks quite stable.
```{r}
pars <- c()
for(i in 1:20){
mod <- depmix(response = winter_pdo ~ 1, data = pdo, nstates = 2)
fitmod <- fit(mod)
pars <- rbind(pars, data.frame(name=c("p11", "p12", "p21", "p22"), value=getpars(fitmod)[3:6]))
}
library(ggplot2)
ggplot(pars, aes(x=value)) + geom_histogram() + xlim(c(0,1)) + facet_wrap(~name)
```

* If you include time varying parameters (e.g. year) in the means of each state, or state transition probabilities, does the model do any better?
 
* Run diagnostics on the best model. Any problems?

    
<br>


2. Bayesian MARSS modelling {#sec-bayes}

Building off what we did for lab last week, we're going to work with the NEON data from Barco Lake, doing simultaneous modeling of oxygen and temperature. For this exercise, we'll only work with the data from the first four months of 2020 (where there was consistent data for each of the responses).

```{r read-data-neon}
library(dplyr)
data(neon_barc, package = "atsalibrary")

# we're going to just work with data from 2020
barc <- neon_barc %>% 
  dplyr::mutate(
    year = lubridate::year(neon_barc$date),
    yday = lubridate::yday(neon_barc$date),
    month = lubridate::month(neon_barc$date)) %>%
  dplyr::filter(year==2020, month < 5) %>%
  dplyr::filter(row_number() %% 2 == 1)
```

If you were to try to fit the temperature and oxygen data using MARSS, you'll notice that it's difficult to get the model to converge. Why? In the absence of covariates, it seems like there's some support for including moving average components. For oxygen in particular, we can see this with the PACF plot,

```{r pacf, echo=FALSE}
pacf((data$oxygen),na.action = na.pass)
```

So instead of fitting this model using a Bayesian version of MARSS, we're going to fit this using a moving average model. We've made a few other changes to the model that we can't do in a Bayesian setting. Most importantly we'll model the MA deviations as a random walk, generated from a multivariate Student-t distribution (if you examine the first differenced temperature or oxygen series, you'll see the data support a heavy tailed distribution). 

Here's the code, that will create a file called "model.stan" in your working directory,

```{r create-model}
model <- cat("data {
  int<lower=0> n; // size of trainging set (with NAs)
  int<lower=0> n_o2; // number of observations
  int o2_x[n_o2];
  real o2_y[n_o2];
  real o2_z[n_o2];  
  real o2_sd[n_o2];
  int<lower=0> n_temp; // number of observations
  int temp_x[n_temp];
  real temp_y[n_temp];
  real temp_sd[n_temp];  
  int n_forecast;
  int n_lag_o2;
  int n_lag_temp;
}
transformed data {
  vector[2] zeros;
  matrix[2,2] identity;
  int lag_o2;
  int lag_temp;  
  zeros = rep_vector(0, 2);
  identity = diag_matrix(rep_vector(1.0,2));

  lag_o2 = 1;
  lag_temp = 1;
  if(n_lag_o2 > 1) lag_o2 = 1;
  if(n_lag_temp > 1) lag_temp = 1;  
}
parameters {
  vector[2] devs[n+n_forecast-1];
  cov_matrix[2] Sigma;
  real<lower=0> o2_est_sd;
  real<lower=0> temp_est_sd;
  real o2_x0[lag_o2]; // initial conditions
  real temp_x0[lag_temp]; // initial conditions
  real u_temp;
  real u_o2;
}
transformed parameters {
  vector[n+n_forecast] o2_pred;
  vector[n_forecast] o2_forecast;
  vector[n+n_forecast] temp_pred;
  vector[n_forecast] temp_forecast;  
  real rho;
  // calc correlation
  rho = Sigma[1,2]/(sqrt(Sigma[1,1])*sqrt(Sigma[2,2]));
  // predictions for first states
  for(t in 1:lag_o2) {
    o2_pred[t] = o2_x0[t];
  }
  for(t in 1:lag_o2) {
    temp_pred[t] = temp_x0[t];
  }  

  for(i in (1+lag_o2):(n+n_forecast)) {
    o2_pred[i] = o2_pred[i-1] + devs[i-1,1];
  }
  for(i in (1+lag_temp):(n+n_forecast)) {
    temp_pred[i] = temp_pred[i-1] + devs[i-1,2];
  }

  // this is redundant but easier to work with output -- creates object o2_forecast
  // containing forecast n_forecast time steps ahead
  for(t in 1:n_forecast) {
    o2_forecast[t] = o2_pred[n_o2+t];
    temp_forecast[t] = temp_pred[n_temp+t];
  }
    
}
model {
  // initial conditions, centered on mean
  o2_x0 ~ normal(7,3);
  temp_x0 ~ normal(22,3);
  o2_est_sd ~ student_t(3,0,2);
  temp_est_sd ~ student_t(3,0,2);
  // df parameter
  devs[1] ~ multi_student_t(3, zeros, Sigma);
  for(i in 2:(n+n_forecast-1)) {
    devs[i] ~ multi_student_t(3, devs[i-1], Sigma);
  }

  // likelihood
  for(t in 1:n_o2) {
    o2_y[t] ~ normal(o2_pred[o2_x[t]], o2_est_sd);
  }
  for(t in 1:n_temp) {
    temp_y[t] ~ normal(temp_pred[temp_x[t]], temp_est_sd);
  }
} 
", file="model.stan")
```

Next we have to create the data. We'll use a similar version of the function that we gave you last week,

```{r}
data$indx <- seq(1, nrow(data)) # dimension of data
n_forecast <- 7 # forecast horizon
n_lag_o2 <- 0 # lag for AR component
n_lag_temp <- 0 # lag for AR component
last_obs <- nrow(data)

create_stan_data <- function(data, last_obs, n_forecast, n_lag_o2, 
    n_lag_temp) {
    # create test data
    o2_test <- dplyr::filter(data, indx %in% seq(last_obs + 1, 
        (last_obs + n_forecast)))
    temp_test <- dplyr::filter(data, indx %in% seq(last_obs + 
        1, (last_obs + n_forecast)))
    
    o2_train <- dplyr::filter(data, indx <= last_obs, !is.na(oxygen), !is.na(depth_oxygen))
    o2_x <- o2_train$indx
    o2_y <- o2_train$oxygen
    o2_z <- o2_train$depth_oxygen
    o2_sd <- o2_train$oxygen_sd
    n_o2 <- nrow(o2_train)
    
    temp_train <- dplyr::filter(data, indx <= last_obs, !is.na(temperature))
    temp_x <- temp_train$indx
    temp_y <- temp_train$temperature
    temp_sd <- temp_train$temperature_sd
    n_temp <- nrow(temp_train)
    
    stan_data <- list(n = last_obs, n_lag_o2 = n_lag_o2, n_lag_temp = n_lag_temp, 
        n_forecast = n_forecast, n_o2 = n_o2, o2_x = o2_x, o2_y = o2_y, o2_z = o2_z,
        o2_sd = o2_sd, n_temp = n_temp, temp_x = temp_x, temp_y = temp_y, 
        temp_sd = temp_sd)
    
    return(list(o2_train = o2_train, temp_train = temp_train, 
        stan_data = stan_data, o2_test = o2_test, temp_test = temp_test))
}

stan_data <- create_stan_data(data, last_obs, n_forecast, n_lag_o2, 
    n_lag_temp)
```


And finally, we can fit the model. The `pars` here includes a list of the initial conditions (`o2_x0`,`temp_x0`), the covariance matrix of the process deviations (`Sigma`), the correlation between oxygen and temperature deviations (as a derived parameter `rho`) and the predicted values of temperature (`temp_pred`) and oxygen (`o2_pred`). 

You'll probably want to just do this once (it may take ~ 15 minutes) and save the object 
```{r eval=FALSE}
fit <- stan(file="model.stan", data=stan_data$stan_data, 
            chains=3, 
            iter=3000,
            pars=c("o2_x0","temp_x0","Sigma","rho","temp_pred","o2_pred"))
pars = extract(fit)
```


    a. Using the interactive 'shinystan', does it appear the model converged? You can be qualitative and brief, with a focus just on the initial conditions and variance parameters. 
    
    b. Similarly, are any of the R-hat values for the parameters from #1 > 1.1? You can see this in the table created by summary(fit)$summary
    
    c. Make a posterior histogram, or density plot, of the derived correlation between oxygen and temperature. Is there support for them being negatively correlated, not correlated, or positively correlated?
    
    d. Make predictions of the estimated temperature and oxygen time series from the model, along with 95% CIs on each. Because of differing scales, these can be on different plots





